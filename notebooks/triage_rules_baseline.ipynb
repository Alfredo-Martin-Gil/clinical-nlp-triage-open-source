{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Triage rules baseline\n",
        "\n",
        "Prototype notebook for rule-based clinical NLP triage.\n",
        "\n",
        "- Loads `data/lexicon_redflags.csv`\n",
        "- Loads `data/notes_synthetic.csv`\n",
        "- Applies weighted rule-based scoring (lexicon `weight`)\n",
        "- Handles simple negation and history/past context\n",
        "- Exports `outputs/predictions.csv`\n",
        "- Evaluates predictions vs. ground-truth `label`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "lexicon = pd.read_csv(\"../data/lexicon_redflags.csv\")\n",
        "notes = pd.read_csv(\"../data/notes_synthetic.csv\")\n",
        "\n",
        "# Guardrails\n",
        "required_lex_cols = {\"term\", \"weight\"}\n",
        "missing_lex = required_lex_cols - set(lexicon.columns)\n",
        "if missing_lex:\n",
        "    raise ValueError(f\"lexicon_redflags.csv missing columns: {sorted(missing_lex)}\")\n",
        "\n",
        "required_notes_cols = {\"id\", \"text\", \"entity\"}\n",
        "missing_notes = required_notes_cols - set(notes.columns)\n",
        "if missing_notes:\n",
        "    raise ValueError(f\"notes_synthetic.csv missing columns: {sorted(missing_notes)}\")\n",
        "\n",
        "# Normalize lexicon\n",
        "lexicon[\"term\"] = lexicon[\"term\"].astype(str).str.strip().str.lower()\n",
        "lexicon[\"weight\"] = pd.to_numeric(lexicon[\"weight\"], errors=\"coerce\").fillna(0)\n",
        "\n",
        "# Keep non-empty terms\n",
        "lexicon = lexicon.loc[lexicon[\"term\"].astype(bool)].copy()\n",
        "\n",
        "# Terms with weights\n",
        "term_weights = dict(zip(lexicon[\"term\"].tolist(), lexicon[\"weight\"].tolist()))\n",
        "terms = list(term_weights.keys())\n",
        "\n",
        "# --- Context handling (baseline, intentionally simple) ---\n",
        "NEGATION_CUES = {\n",
        "    \"no\", \"not\", \"denies\", \"deny\", \"denied\", \"without\", \"none\", \"negative for\",\n",
        "    \"sin\", \"niega\", \"negativo\", \"ausencia de\"\n",
        "}\n",
        "\n",
        "HISTORY_CUES = {\n",
        "    \"history of\", \"hx of\", \"h/o\", \"previous\", \"prior\", \"years ago\", \"months ago\",\n",
        "    \"antecedent\", \"antecedentes\", \"historia\", \"hace años\", \"hace meses\", \"previo\"\n",
        "}\n",
        "\n",
        "def _tokenize(s: str):\n",
        "    return re.findall(r\"[a-zA-Záéíóúñü]+\", str(s).lower())\n",
        "\n",
        "def _has_history_context(text: str, window_text: str) -> bool:\n",
        "    t = str(text).lower()\n",
        "    w = str(window_text).lower()\n",
        "    return any(cue in t for cue in HISTORY_CUES) or any(cue in w for cue in HISTORY_CUES)\n",
        "\n",
        "def _is_negated_near(text: str, term: str, window_tokens: int = 6) -> bool:\n",
        "    \"\"\"Return True if a negation cue appears within a token window before the term occurrence.\"\"\"\n",
        "    s = str(text).lower()\n",
        "\n",
        "    # Try to find the first occurrence (baseline)\n",
        "    idx = s.find(term)\n",
        "    if idx == -1:\n",
        "        return False\n",
        "\n",
        "    # Take a left window around the match\n",
        "    left = s[max(0, idx - 120):idx]  # char window\n",
        "    toks = _tokenize(left)\n",
        "    tail = toks[-window_tokens:] if len(toks) >= window_tokens else toks\n",
        "    tail_str = \" \".join(tail)\n",
        "\n",
        "    # Direct token cues\n",
        "    if any(cue in tail for cue in NEGATION_CUES):\n",
        "        return True\n",
        "    # Phrase cues\n",
        "    if any(cue in tail_str for cue in NEGATION_CUES):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def score_text(text: str):\n",
        "    \"\"\"Weighted score from lexicon terms; drops negated matches; downweights history/past.\"\"\"\n",
        "    s = str(text).lower()\n",
        "    score = 0.0\n",
        "    matched = []\n",
        "\n",
        "    for term in terms:\n",
        "        if not term:\n",
        "            continue\n",
        "        if term in s:\n",
        "            # Negation gate\n",
        "            if _is_negated_near(s, term):\n",
        "                continue\n",
        "\n",
        "            w = float(term_weights.get(term, 0.0))\n",
        "\n",
        "            # History/past downweight (baseline heuristic)\n",
        "            # If the whole note or local region suggests past history, penalize by 50%.\n",
        "            idx = s.find(term)\n",
        "            local = s[max(0, idx - 80): idx + len(term) + 80]\n",
        "            if _has_history_context(s, local):\n",
        "                w *= 0.5\n",
        "\n",
        "            score += w\n",
        "            matched.append((term, w))\n",
        "\n",
        "    return score, matched\n",
        "\n",
        "def predict_label_from_score(score: float) -> str:\n",
        "    # Baseline thresholds (tunable):\n",
        "    # - high if >= 8 (typically >=2 strong red flags)\n",
        "    # - intermediate if 4..7.5\n",
        "    # - low otherwise\n",
        "    if score >= 8:\n",
        "        return \"high\"\n",
        "    if score >= 4:\n",
        "        return \"intermediate\"\n",
        "    return \"low\"\n",
        "\n",
        "# Apply\n",
        "scored = notes[\"text\"].apply(score_text)\n",
        "notes[\"score\"] = scored.apply(lambda x: x[0])\n",
        "notes[\"matched_terms\"] = scored.apply(lambda x: x[1])\n",
        "notes[\"predicted_label\"] = notes[\"score\"].apply(predict_label_from_score)\n",
        "\n",
        "# Export (include score to debug)\n",
        "notes[[\"id\", \"text\", \"entity\", \"score\", \"predicted_label\"]].to_csv(\n",
        "    \"../outputs/predictions.csv\", index=False\n",
        ")\n",
        "\n",
        "print(\"Saved outputs/predictions.csv\")\n",
        "print(\"\\nScore distribution:\")\n",
        "print(notes[\"score\"].describe().to_string())\n",
        "print(\"\\nLabel distribution:\")\n",
        "print(notes[\"predicted_label\"].value_counts(dropna=False).to_string())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick evaluation (baseline)\n",
        "\n",
        "This evaluates the weighted baseline against the synthetic ground truth `label`.\n",
        "\n",
        "- Overall accuracy\n",
        "- Confusion matrix (all entities)\n",
        "- Accuracy by entity\n",
        "- A few mismatches to inspect\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardrails: only run evaluation if ground-truth label exists\n",
        "if \"label\" not in notes.columns:\n",
        "    raise ValueError(\"notes_synthetic.csv is missing required column: 'label'\")\n",
        "\n",
        "# Overall accuracy\n",
        "acc = (notes[\"predicted_label\"] == notes[\"label\"]).mean()\n",
        "print(f\"Overall accuracy: {acc:.3f} ({int((notes['predicted_label'] == notes['label']).sum())}/{len(notes)})\")\n",
        "\n",
        "# Confusion matrix\n",
        "print(\"\\nConfusion matrix (label x predicted_label):\")\n",
        "cm = pd.crosstab(\n",
        "    notes[\"label\"],\n",
        "    notes[\"predicted_label\"],\n",
        "    rownames=[\"label\"],\n",
        "    colnames=[\"predicted_label\"],\n",
        "    dropna=False,\n",
        ")\n",
        "print(cm.to_string())\n",
        "\n",
        "# Accuracy by entity\n",
        "print(\"\\nAccuracy by entity:\")\n",
        "by_entity = notes.groupby(\"entity\").apply(lambda df: (df[\"predicted_label\"] == df[\"label\"]).mean())\n",
        "by_entity = by_entity.sort_values(ascending=False)\n",
        "print(by_entity.to_string())\n",
        "\n",
        "# Show a few mismatches for inspection\n",
        "mismatches = notes.loc[\n",
        "    notes[\"predicted_label\"] != notes[\"label\"],\n",
        "    [\"id\", \"entity\", \"text\", \"label\", \"predicted_label\", \"score\", \"matched_terms\"],\n",
        "]\n",
        "print(\"\\nSample mismatches (first 15):\")\n",
        "print(mismatches.head(15).to_string(index=False))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
