{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Triage rules baseline\n",
        "\n",
        "Prototype notebook for rule-based clinical NLP triage.\n",
        "\n",
        "- Loads `data/lexicon_redflags.csv`\n",
        "- Loads `data/notes_synthetic.csv`\n",
        "- Applies rule-based scoring (token matching + minimal context filters)\n",
        "- Exports `outputs/predictions.csv`\n",
        "- Evaluates predictions vs. ground-truth `label`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "lexicon = pd.read_csv(\"../data/lexicon_redflags.csv\")\n",
        "notes = pd.read_csv(\"../data/notes_synthetic.csv\")\n",
        "\n",
        "# Basic sanitation\n",
        "terms_raw = [str(t) for t in lexicon[\"term\"].dropna().tolist()]\n",
        "\n",
        "def tokenize(text: str) -> list[str]:\n",
        "    \"\"\"Lowercase + alphanumeric tokenization.\"\"\"\n",
        "    s = str(text).lower()\n",
        "    return re.findall(r\"[a-z0-9]+\", s)\n",
        "\n",
        "def tokenize_term(term: str) -> list[str]:\n",
        "    return tokenize(term)\n",
        "\n",
        "# Pre-tokenize lexicon terms (keep only non-empty)\n",
        "TERMS = []\n",
        "for t in terms_raw:\n",
        "    tt = tokenize_term(t)\n",
        "    if tt:\n",
        "        TERMS.append((t, tt))  # (original, tokens)\n",
        "\n",
        "# Minimal context cues (EN only for now)\n",
        "NEGATION_CUES = {\n",
        "    \"no\", \"not\", \"denies\", \"denied\", \"without\", \"negative\"\n",
        "}\n",
        "\n",
        "# Multi-token cues handled by matching token sequences\n",
        "NEGATION_PHRASES = [\n",
        "    [\"negative\", \"for\"],\n",
        "    [\"free\", \"of\"],\n",
        "    [\"rule\", \"out\"],\n",
        "]\n",
        "\n",
        "HISTORICAL_CUES = {\n",
        "    \"history\", \"previous\", \"prior\", \"past\", \"resolved\", \"remote\"\n",
        "}\n",
        "\n",
        "HISTORICAL_PHRASES = [\n",
        "    [\"years\", \"ago\"],\n",
        "    [\"last\", \"year\"],\n",
        "    [\"in\", \"the\", \"past\"],\n",
        "    [\"hx\", \"of\"],\n",
        "    [\"history\", \"of\"],\n",
        "]\n",
        "\n",
        "def find_subsequence_positions(tokens: list[str], sub: list[str]) -> list[int]:\n",
        "    \"\"\"Return start indices where sub occurs in tokens.\"\"\"\n",
        "    if not sub or len(sub) > len(tokens):\n",
        "        return []\n",
        "    hits = []\n",
        "    L = len(sub)\n",
        "    for i in range(0, len(tokens) - L + 1):\n",
        "        if tokens[i:i+L] == sub:\n",
        "            hits.append(i)\n",
        "    return hits\n",
        "\n",
        "def window_tokens(tokens: list[str], start: int, end: int) -> list[str]:\n",
        "    start = max(0, start)\n",
        "    end = min(len(tokens), end)\n",
        "    return tokens[start:end]\n",
        "\n",
        "def has_phrase(window: list[str], phrases: list[list[str]]) -> bool:\n",
        "    for ph in phrases:\n",
        "        if find_subsequence_positions(window, ph):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def is_negated(tokens: list[str], term_start: int, term_len: int, window: int = 5) -> bool:\n",
        "    \"\"\"Check for negation cues immediately before the term (simple window).\"\"\"\n",
        "    w = window_tokens(tokens, term_start - window, term_start)\n",
        "    if any(t in NEGATION_CUES for t in w):\n",
        "        return True\n",
        "    if has_phrase(w, NEGATION_PHRASES):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def is_historical(tokens: list[str], term_start: int, term_len: int, window: int = 8) -> bool:\n",
        "    \"\"\"Check for historical cues in a wider window before the term.\"\"\"\n",
        "    w = window_tokens(tokens, term_start - window, term_start)\n",
        "    if any(t in HISTORICAL_CUES for t in w):\n",
        "        return True\n",
        "    if has_phrase(w, HISTORICAL_PHRASES):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def count_hits_with_filters(text: str) -> dict:\n",
        "    \"\"\"Count lexicon hits using token matching, excluding negated/historical contexts.\"\"\"\n",
        "    tokens = tokenize(text)\n",
        "\n",
        "    raw_hits = 0\n",
        "    filtered_hits = 0\n",
        "    negated_hits = 0\n",
        "    historical_hits = 0\n",
        "\n",
        "    # For each term, count at most once per note (keeps baseline conservative)\n",
        "    for original, tt in TERMS:\n",
        "        positions = find_subsequence_positions(tokens, tt)\n",
        "        if not positions:\n",
        "            continue\n",
        "\n",
        "        # If term appears multiple times, we treat any non-negated & non-historical occurrence as a valid hit\n",
        "        raw_hits += 1\n",
        "        term_len = len(tt)\n",
        "\n",
        "        valid = False\n",
        "        any_neg = False\n",
        "        any_hist = False\n",
        "        for pos in positions:\n",
        "            neg = is_negated(tokens, pos, term_len)\n",
        "            hist = is_historical(tokens, pos, term_len)\n",
        "            if neg:\n",
        "                any_neg = True\n",
        "            if hist:\n",
        "                any_hist = True\n",
        "            if (not neg) and (not hist):\n",
        "                valid = True\n",
        "                break\n",
        "\n",
        "        if valid:\n",
        "            filtered_hits += 1\n",
        "        else:\n",
        "            if any_neg:\n",
        "                negated_hits += 1\n",
        "            if any_hist:\n",
        "                historical_hits += 1\n",
        "\n",
        "    return {\n",
        "        \"hits_raw\": raw_hits,\n",
        "        \"hits_filtered\": filtered_hits,\n",
        "        \"hits_negated\": negated_hits,\n",
        "        \"hits_historical\": historical_hits,\n",
        "    }\n",
        "\n",
        "def predict_label_from_hits(hits: int) -> str:\n",
        "    if hits >= 2:\n",
        "        return \"high\"\n",
        "    elif hits == 1:\n",
        "        return \"intermediate\"\n",
        "    else:\n",
        "        return \"low\"\n",
        "\n",
        "stats = notes[\"text\"].apply(count_hits_with_filters).apply(pd.Series)\n",
        "notes = pd.concat([notes, stats], axis=1)\n",
        "notes[\"predicted_label\"] = notes[\"hits_filtered\"].apply(predict_label_from_hits)\n",
        "\n",
        "notes[[\"id\", \"text\", \"entity\", \"label\", \"hits_raw\", \"hits_filtered\", \"hits_negated\", \"hits_historical\", \"predicted_label\"]].to_csv(\n",
        "    \"../outputs/predictions.csv\", index=False\n",
        ")\n",
        "\n",
        "print(\"Saved ../outputs/predictions.csv\")\n",
        "print(\"\\nLabel distribution (predicted):\")\n",
        "print(notes[\"predicted_label\"].value_counts(dropna=False).to_string())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick evaluation (baseline)\n",
        "\n",
        "This evaluates the baseline against the synthetic ground truth `label`.\n",
        "\n",
        "- Overall accuracy\n",
        "- Confusion matrix (all entities)\n",
        "- Accuracy by entity\n",
        "- A few mismatches to inspect (with hit diagnostics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardrails: only run evaluation if ground-truth label exists\n",
        "if \"label\" not in notes.columns:\n",
        "    raise ValueError(\"notes_synthetic.csv is missing required column: 'label'\")\n",
        "\n",
        "# Overall accuracy\n",
        "acc = (notes[\"predicted_label\"] == notes[\"label\"]).mean()\n",
        "print(f\"Overall accuracy: {acc:.3f} ({int((notes['predicted_label'] == notes['label']).sum())}/{len(notes)})\")\n",
        "\n",
        "# Confusion matrix\n",
        "print(\"\\nConfusion matrix (label x predicted_label):\")\n",
        "cm = pd.crosstab(\n",
        "    notes[\"label\"],\n",
        "    notes[\"predicted_label\"],\n",
        "    rownames=[\"label\"],\n",
        "    colnames=[\"predicted_label\"],\n",
        "    dropna=False,\n",
        ")\n",
        "print(cm.to_string())\n",
        "\n",
        "# Accuracy by entity\n",
        "print(\"\\nAccuracy by entity:\")\n",
        "by_entity = notes.groupby(\"entity\").apply(lambda df: (df[\"predicted_label\"] == df[\"label\"]).mean())\n",
        "by_entity = by_entity.sort_values(ascending=False)\n",
        "print(by_entity.to_string())\n",
        "\n",
        "# Show a few mismatches for inspection\n",
        "cols = [\"id\", \"entity\", \"text\", \"label\", \"predicted_label\", \"hits_raw\", \"hits_filtered\", \"hits_negated\", \"hits_historical\"]\n",
        "mismatches = notes.loc[notes[\"predicted_label\"] != notes[\"label\"], cols]\n",
        "print(\"\\nSample mismatches (first 20):\")\n",
        "print(mismatches.head(20).to_string(index=False))\n",
        "\n",
        "# Optional: quick look at cases where raw hits were reduced by filters\n",
        "reduced = notes.loc[notes[\"hits_raw\"] > notes[\"hits_filtered\"], cols]\n",
        "print(\"\\nSample reduced-by-filters (first 20):\")\n",
        "print(reduced.head(20).to_string(index=False))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
