{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Triage rules baseline\n",
        "\n",
        "Prototype notebook for rule-based clinical NLP triage.\n",
        "\n",
        "- Loads `data/lexicon_redflags.csv`\n",
        "- Loads `data/notes_synthetic.csv`\n",
        "- Applies simple rule-based scoring\n",
        "- Exports `outputs/predictions.csv`\n",
        "- Evaluates predictions vs. ground-truth `label`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "lexicon = pd.read_csv(\"../data/lexicon_redflags.csv\")\n",
        "notes = pd.read_csv(\"../data/notes_synthetic.csv\")\n",
        "\n",
        "terms = [str(t).lower() for t in lexicon[\"term\"].tolist()]\n",
        "\n",
        "def predict_label(text):\n",
        "    text = str(text).lower()\n",
        "    hits = sum(1 for t in terms if t and t in text)\n",
        "    if hits >= 2:\n",
        "        return \"high\"\n",
        "    elif hits == 1:\n",
        "        return \"intermediate\"\n",
        "    else:\n",
        "        return \"low\"\n",
        "\n",
        "notes[\"predicted_label\"] = notes[\"text\"].apply(predict_label)\n",
        "\n",
        "notes[[\"id\", \"text\", \"entity\", \"predicted_label\"]].to_csv(\n",
        "    \"../outputs/predictions.csv\", index=False\n",
        ")\n",
        "\n",
        "print(\"Saved outputs/predictions.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick evaluation (baseline)\n",
        "\n",
        "This evaluates the simple substring-matching baseline against the synthetic ground truth `label`.\n",
        "\n",
        "- Overall accuracy\n",
        "- Confusion matrix (all entities)\n",
        "- Accuracy by entity\n",
        "- A few mismatches to inspect\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardrails: only run evaluation if ground-truth label exists\n",
        "if \"label\" not in notes.columns:\n",
        "    raise ValueError(\"notes_synthetic.csv is missing required column: 'label'\")\n",
        "\n",
        "# Overall accuracy\n",
        "acc = (notes[\"predicted_label\"] == notes[\"label\"]).mean()\n",
        "print(f\"Overall accuracy: {acc:.3f} ({int((notes['predicted_label'] == notes['label']).sum())}/{len(notes)})\")\n",
        "\n",
        "# Confusion matrix\n",
        "print(\"\\nConfusion matrix (label x predicted_label):\")\n",
        "cm = pd.crosstab(notes[\"label\"], notes[\"predicted_label\"], rownames=[\"label\"], colnames=[\"predicted_label\"], dropna=False)\n",
        "print(cm.to_string())\n",
        "\n",
        "# Accuracy by entity\n",
        "print(\"\\nAccuracy by entity:\")\n",
        "by_entity = notes.groupby(\"entity\").apply(lambda df: (df[\"predicted_label\"] == df[\"label\"]).mean())\n",
        "by_entity = by_entity.sort_values(ascending=False)\n",
        "print(by_entity.to_string())\n",
        "\n",
        "# Show a few mismatches for inspection\n",
        "mismatches = notes.loc[notes[\"predicted_label\"] != notes[\"label\"], [\"id\", \"entity\", \"text\", \"label\", \"predicted_label\"]]\n",
        "print(\"\\nSample mismatches (first 15):\")\n",
        "print(mismatches.head(15).to_string(index=False))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
