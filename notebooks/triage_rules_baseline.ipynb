{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Triage rules baseline\n",
        "\n",
        "Prototype notebook for rule-based clinical NLP triage.\n",
        "\n",
        "- Loads `data/lexicon_redflags.csv`\n",
        "- Loads `data/notes_synthetic.csv`\n",
        "- Applies simple rule-based scoring (substring baseline)\n",
        "- Adds minimal negation handling (heuristic)\n",
        "- Exports `outputs/predictions.csv`\n",
        "- Evaluates predictions vs. ground-truth `label`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "lexicon = pd.read_csv(\"../data/lexicon_redflags.csv\")\n",
        "notes = pd.read_csv(\"../data/notes_synthetic.csv\")\n",
        "\n",
        "# Normalize lexicon terms (keep as phrases)\n",
        "terms = (\n",
        "    lexicon[\"term\"].dropna().astype(str).str.strip().str.lower().tolist()\n",
        ")\n",
        "\n",
        "# Minimal negation lexicon (heuristic)\n",
        "# Goal: avoid counting obvious \"no/denies/without\" near a term\n",
        "NEGATIONS = {\n",
        "    \"no\",\n",
        "    \"not\",\n",
        "    \"denies\",\n",
        "    \"denied\",\n",
        "    \"without\",\n",
        "    \"negative\",\n",
        "    \"neg\",\n",
        "    \"never\",\n",
        "}\n",
        "\n",
        "TOKEN_RE = re.compile(r\"[a-z0-9']+\")\n",
        "\n",
        "def tokenize(text: str) -> list[str]:\n",
        "    return TOKEN_RE.findall(str(text).lower())\n",
        "\n",
        "def phrase_in_tokens_at(tokens: list[str], phrase_tokens: list[str]) -> list[int]:\n",
        "    \"\"\"Return start indices where phrase_tokens match tokens contiguously.\"\"\"\n",
        "    if not phrase_tokens or not tokens:\n",
        "        return []\n",
        "    n = len(phrase_tokens)\n",
        "    hits = []\n",
        "    for i in range(0, len(tokens) - n + 1):\n",
        "        if tokens[i : i + n] == phrase_tokens:\n",
        "            hits.append(i)\n",
        "    return hits\n",
        "\n",
        "def is_negated(tokens: list[str], start_idx: int, window: int = 3) -> bool:\n",
        "    \"\"\"Heuristic: if a negation token appears within `window` tokens BEFORE the phrase start.\"\"\"\n",
        "    left = max(0, start_idx - window)\n",
        "    context = tokens[left:start_idx]\n",
        "    return any(t in NEGATIONS for t in context)\n",
        "\n",
        "def count_hits_with_negation(text: str, terms: list[str]) -> tuple[int, int]:\n",
        "    \"\"\"Return (positive_hits, negated_hits).\"\"\"\n",
        "    tokens = tokenize(text)\n",
        "    pos_hits = 0\n",
        "    neg_hits = 0\n",
        "\n",
        "    for term in terms:\n",
        "        if not term:\n",
        "            continue\n",
        "        phrase_tokens = tokenize(term)\n",
        "        # Fallback: if tokenization empties it, skip\n",
        "        if not phrase_tokens:\n",
        "            continue\n",
        "\n",
        "        # Find all occurrences of the term as a phrase\n",
        "        starts = phrase_in_tokens_at(tokens, phrase_tokens)\n",
        "        if not starts:\n",
        "            continue\n",
        "\n",
        "        # Count each term at most once per note (baseline behavior)\n",
        "        # If any occurrence is non-negated -> count as positive\n",
        "        any_pos = False\n",
        "        any_neg = False\n",
        "        for s in starts:\n",
        "            if is_negated(tokens, s, window=3):\n",
        "                any_neg = True\n",
        "            else:\n",
        "                any_pos = True\n",
        "\n",
        "        if any_pos:\n",
        "            pos_hits += 1\n",
        "        elif any_neg:\n",
        "            neg_hits += 1\n",
        "\n",
        "    return pos_hits, neg_hits\n",
        "\n",
        "def predict_label_from_hits(hits: int) -> str:\n",
        "    if hits >= 2:\n",
        "        return \"high\"\n",
        "    elif hits == 1:\n",
        "        return \"intermediate\"\n",
        "    else:\n",
        "        return \"low\"\n",
        "\n",
        "# Apply scoring\n",
        "hit_counts = notes[\"text\"].apply(lambda t: count_hits_with_negation(t, terms))\n",
        "notes[\"hits_count\"] = hit_counts.apply(lambda x: x[0])\n",
        "notes[\"negated_hits_count\"] = hit_counts.apply(lambda x: x[1])\n",
        "notes[\"predicted_label\"] = notes[\"hits_count\"].apply(predict_label_from_hits)\n",
        "\n",
        "# Export\n",
        "notes[[\"id\", \"text\", \"entity\", \"hits_count\", \"negated_hits_count\", \"predicted_label\"]].to_csv(\n",
        "    \"../outputs/predictions.csv\", index=False\n",
        ")\n",
        "\n",
        "print(\"Saved ../outputs/predictions.csv\")\n",
        "print(\"\\nNegation heuristic: ignores terms when a negation token appears within 3 tokens before the phrase.\")\n",
        "print(\"Total positive hits:\", int(notes[\"hits_count\"].sum()))\n",
        "print(\"Total negated hits:\", int(notes[\"negated_hits_count\"].sum()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick evaluation (baseline)\n",
        "\n",
        "This evaluates the simple baseline against the synthetic ground truth `label`.\n",
        "\n",
        "- Overall accuracy\n",
        "- Confusion matrix (all entities)\n",
        "- Accuracy by entity\n",
        "- A few mismatches to inspect\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardrails: only run evaluation if ground-truth label exists\n",
        "if \"label\" not in notes.columns:\n",
        "    raise ValueError(\"notes_synthetic.csv is missing required column: 'label'\")\n",
        "\n",
        "# Overall accuracy\n",
        "acc = (notes[\"predicted_label\"] == notes[\"label\"]).mean()\n",
        "print(f\"Overall accuracy: {acc:.3f} ({int((notes['predicted_label'] == notes['label']).sum())}/{len(notes)})\")\n",
        "\n",
        "# Confusion matrix\n",
        "print(\"\\nConfusion matrix (label x predicted_label):\")\n",
        "cm = pd.crosstab(notes[\"label\"], notes[\"predicted_label\"], rownames=[\"label\"], colnames=[\"predicted_label\"], dropna=False)\n",
        "print(cm.to_string())\n",
        "\n",
        "# Accuracy by entity\n",
        "print(\"\\nAccuracy by entity:\")\n",
        "by_entity = notes.groupby(\"entity\").apply(lambda df: (df[\"predicted_label\"] == df[\"label\"]).mean())\n",
        "by_entity = by_entity.sort_values(ascending=False)\n",
        "print(by_entity.to_string())\n",
        "\n",
        "# Show a few mismatches for inspection\n",
        "mismatches = notes.loc[notes[\"predicted_label\"] != notes[\"label\"], [\"id\", \"entity\", \"text\", \"label\", \"predicted_label\", \"hits_count\", \"negated_hits_count\"]]\n",
        "print(\"\\nSample mismatches (first 15):\")\n",
        "print(mismatches.head(15).to_string(index=False))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
